from fastapi import FastAPI
from pydantic import BaseModel
from openai import OpenAI
from fastapi.middleware.cors import CORSMiddleware

CUSTOMER_SERVICE = "๐ +967 784784118 / +967 775436959"
'''
RULE_BASED_ANSWERS = {
    
    # ุชุญูุฉ
    "ูุฑุญุจุง": "ุฃููุงู ูุณููุงู ุจู ูู ูุชุฌุฑ ููุฑุง. ููู ุฃูุฏุฑ ุฃุณุงุนุฏูุ",
    "ุงูุณูุงู": "ูุฑุญุจูุง ุจู ูู ูุชุฌุฑ ููุฑุง. ููู ุฃูุฏุฑ ุฃุณุงุนุฏูุ",
    "ุงููุง": "ุฃููุงู ุจู ูู ูุชุฌุฑ ููุฑุง. ููู ุฃูุฏุฑ ุฃุณุงุนุฏูุ",

    # ุงูุดุญู
    "ูุชู ูุทูุน ุงูุดุญู": "ูุชู ุดุญู ุงูุทูุจ ุฎูุงู 24 ุฅูู 48 ุณุงุนุฉ ุนูู.",
    "ูุชู ุณูุชู ุดุญู": "ูุชู ุดุญู ุงูุทูุจ ุฎูุงู 24 ุฅูู 48 ุณุงุนุฉ ุนูู.",
    "ููุด ุงูุทูุจ ูุชุฃุฎุฑ": f"ูุนุชุฐุฑ ุนู ุงูุชุฃุฎูุฑ. ุฒูุฏูุง ุจุฑูู ุงูุทูุจ ุฃู ุชูุงุตู ูุนูุง: {CUSTOMER_SERVICE}",
    "ููุด ุงูุดุญู ูุชุฃุฎุฑ": f"ูุนุชุฐุฑ ุนู ุงูุชุฃุฎูุฑ. ุฒูุฏูุง ุจุฑูู ุงูุทูุจ ุฃู ุชูุงุตู ูุนูุง: {CUSTOMER_SERVICE}",

    # ุฎุฏูุฉ ุงูุนููุงุก
    "ุฑูู ุฎุฏูุฉ ุงูุนููุงุก": f"ุฑูู ุฎุฏูุฉ ุงูุนููุงุก:\n{CUSTOMER_SERVICE}",
    "ุงููู ุฎุฏูุฉ ุงูุนููุงุก": f"ุชูุงุตู ูุนูุง ุนุจุฑ:\n{CUSTOMER_SERVICE}",
    "ุงุจุบู ุงููู ุฎุฏูุฉ ุงูุนููุงุก": f"ุชูุงุตู ูุนูุง ุนุจุฑ:\n{CUSTOMER_SERVICE}",
    "ุนูุฏู ูุดููุฉ": f"ูุถุญ ุงููุดููุฉ ุจุงุฎุชุตุงุฑุ ุฃู ุชูุงุตู ูุนูุง: {CUSTOMER_SERVICE}",
}

def rule_based_reply(message: str):
    msg = (message or "").strip().lower()
    for key, reply in RULE_BASED_ANSWERS.items():
        if key in msg:
            return reply
    return None
'''


INTENTS = {
    "shipping_time": {
        "keywords": ["ุดุญู", "ููุดุญู", "ูุทูุน ุงูุดุญู", "ุงูุชูุตูู", "ูุชู"],
        "answer": "ูุชู ุดุญู ุงูุทูุจ ุฎูุงู 24 ุฅูู 48 ุณุงุนุฉ ุนูู."
    },
    "shipping_delay": {
        "keywords": ["ูุชุฃุฎุฑ", "ููุด", "ุชุฃุฎุฑ", "ุชุงุฎุฑ"],
        "answer": "ูุนุชุฐุฑ ุนู ุงูุชุฃุฎูุฑุ ูุฑุฌู ุชุฒููุฏูุง ุจุฑูู ุงูุทูุจ ููุชุงุจุนุฉ ุงูุญุงูุฉ."
    },
    "contact": {
        "keywords": ["ุฎุฏูุฉ ุงูุนููุงุก", "ุงููู", "ุงุชูุงุตู", "ุฑูู"],
        "answer": f"ููููู ุงูุชูุงุตู ูุน ุฎุฏูุฉ ุงูุนููุงุก ุนุจุฑ:\n{CUSTOMER_SERVICE}"
    },
}

def detect_intent(user_text: str):
    text = user_text.lower()
    for intent, data in INTENTS.items():
        for kw in data["keywords"]:
            if kw in text:
                return data["answer"]
    return None

# =====================
# FastAPI App
# =====================
app = FastAPI(title="Noura Chat API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ูุงุญููุง ููููุฏูุง
    allow_methods=["*"],
    allow_headers=["*"],
)

# =====================
# OpenAI Client
# =====================
import os
from dotenv import load_dotenv

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

SYSTEM_PROMPT = """
ุฃูุช ูุณุงุนุฏ ุฐูู ุฑุณูู ููุชุฌุฑ ุฅููุชุฑููู ุงุณูู (ูุชุฌุฑ ููุฑุง).

ููุงูู:
- ูุณุงุนุฏุฉ ุงูุนููุงุก ูู ุฃุณุฆูุฉ ุงูุดุญูุ ุงูุทูุจุงุชุ ุงูุฏูุนุ ุงูุฅุฑุฌุงุนุ ุงูุญุณุงุจุ ุงูุนุฑูุถุ ูุงูุดูุงูู.
- ุงูุฑุฏ ุจุงููุบุฉ ุงูุนุฑุจูุฉ ุจุฃุณููุจ ูุฏูุฏ ููููู.
- ุชูุฏูู ุฅุฌุงุจุงุช ูุฎุชุตุฑุฉ ููุงุถุญุฉ ุจุฏูู ุญุดู.
- ุนุฏู ุงุฎุชุฑุงุน ูุนูููุงุช ุฃู ุณูุงุณุงุช ุบูุฑ ูุฐููุฑุฉ.
- ุฅุฐุง ูุงู ุงูุณุคุงู ุบูุฑ ูุงุถุญุ ุงุทูุจ ุชูุถูุญู ุจูุทู.
- ุฅุฐุง ุงุญุชุงุฌ ุงูุนููู ุฏุนููุง ุจุดุฑููุงุ ูุฌููู ูุฎุฏูุฉ ุงูุนููุงุก.

ูุนูููุงุช ุงููุชุฌุฑ:
- ุงุณู ุงููุชุฌุฑ: ูุชุฌุฑ ููุฑุง
- ุจูุฏ ุงูุนูู: ุงูููู
- ูุฏุฉ ุงูุดุญู ุงููุนุชุงุฏุฉ: 24 ุฅูู 48 ุณุงุนุฉ ุนูู
- ุฃููุงุช ุงูุฏุนู: ูููููุง ูู 9 ุตุจุงุญูุง ุฅูู 9 ูุณุงุกู


ุฃุณููุจ ุงูุฑุฏ:
- ุงุณุชุฎุฏู ูุบุฉ ูุญุชุฑูุฉ ููุฑูุจุฉ ูู ุงูุนููู.
- ูุง ุชุฐูุฑ ุฃูู ูููุฐุฌ ุฐูุงุก ุงุตุทูุงุนู.
- ูุง ุชูุฑุฑ ุงูุณุคุงู.
- ูุง ุชุณุชุฎุฏู ุฑููุฒ ูุซูุฑุฉ.
- ูุง ุชุนุทู ุฅุฌุงุจุฉ ุทูููุฉ ุฅูุง ุฅุฐุง ุทููุจ ุฐูู.
- ุนูุฏ ุงูุชุญูุฉ: ุฑุญูุจ ุจุงูุนููู ูุนุฑูู ุจููุณู ุจุงุฎุชุตุงุฑ.
- ุนูุฏ ุงูุดููู: ุงุนุชุฐุฑ ุฃูููุง ุซู ุงูุชุฑุญ ุงูุญู ุฃู ุงูุชุตุนูุฏ.
- ุนูุฏ ุนุฏู ุชููุฑ ุฅุฌุงุจุฉ ุฏูููุฉ: ุงุทูุจ ุฑูู ุงูุทูุจ ุฃู ูุฌูู ููุฏุนู.

โ ููู:
ุฅุฐุง ูุงู ุงูุณุคุงู ุฎุงุฑุฌ ูุทุงู ุงููุชุฌุฑ (ุณูุงุณุฉุ ุฏููุ ุตุญุฉุ ููุงุถูุน ุนุงูุฉ):
ุงุนุชุฐุฑ ุจูุทู ููุฌูู ุงูุนููู ุฅูู ุฎุฏูุฉ ุงูุนููุงุก ููุท.

"""

# =====================
# Request Schema
# =====================
class ChatRequest(BaseModel):
    message: str

# =====================
# Routes
# =====================
@app.get("/")
def root():
    return {"status": "Noura Chat API is running"}
'''
@app.post("/chat")
def chat(req: ChatRequest):
    user_msg = req.message

    # (1) ุฑุฏูุฏ ุฌุงูุฒุฉ ุฃููุงู
    rule_reply = rule_based_reply(user_msg)
    if rule_reply:
        return {"answer": rule_reply, "source": "rule"}

    # (2) ุฅุฐุง ูุง ููููุง ุฑุฏ ุฌุงูุฒ โ ูุณุชุฎุฏู GPT
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_msg}
        ],
        max_tokens=200,
        temperature=0.2
    )

    return {
        "answer": response.choices[0].message.content,
        "source": "llm"
    }
'''
@app.post("/chat")
def chat(req: ChatRequest):

    # 1๏ธโฃ ูุญุงูู ูุฑุฏ ูู ุงูููุงุนุฏ
    intent_answer = detect_intent(req.message)
    if intent_answer:
        return {
            "answer": intent_answer,
            "source": "rule"
        }

    # 2๏ธโฃ ุฅุฐุง ูุง ููููุงุ ูุฑูุญ ูู GPT
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": req.message}
        ],
        max_tokens=200,
        temperature=0.2
    )

    return {
        "answer": response.choices[0].message.content,
        "source": "llm"
    }

